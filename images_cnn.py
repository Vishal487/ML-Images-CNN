# -*- coding: utf-8 -*-
"""Images:CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rNj9HpVUvffjrAnc6wUv5c2immZ9XeyK

# Convolutional Neural Network (CNN)
"""

import tensorflow as tf
import matplotlib.pyplot as plt

"""### Download and prepare the CIFAR10 dataset
The CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.
"""

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()

"""**Normalizing the dataset**

So that pixel values lies b/w 0 and 1
"""

train_images, test_images = train_images/255.0, test_images/255.0

train_images.shape, test_images.shape

len(train_images), len(test_images)

train_images[0]

train_labels.shape

train_labels[0]

"""**so `labels` are an `array`**"""

train_labels[0][0]

"""**Visualize the data**"""

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i])
    plt.xlabel(class_names[train_labels[i][0]])
plt.show()

"""Hmmm very bad quality

## Create the convolutional base
"""

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)))
model.add(tf.keras.layers.MaxPooling2D((2,2)))
model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D((2,2)))
model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))

model.summary()

"""The output of every `Conv2D` and `MaxPooling2D` layer is a `3D tensor` of shape (`height, width, channels`). The width and height dimensions tend to **shrink** as we go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64)
(see the third dimension of output shape)

## Add Dense layers on top

Note that the output shape of last conv2D layer is (4, 4, 64). 

Dense layer take vector as input (`which are 1D`), while current output (of last conv2D layer) is a 3D tensor. First we need to `flatten (or unroll)` the 3D output to 1D, then we may add one or more Dense layers ont top. Since we have 10 diff. output class final Dense layer should have 10 outputs and a `softmax` activation.
"""

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.summary()

"""**Note** that (4,4,64) shape outputs were flattened into vectors of shape (1024) before going through two Dense layers

## Compile and train the model
"""

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_images, train_labels,
                    epochs=10,
                    validation_data=(test_images, test_labels))

history.history

"""Let's make it more visualizable"""

plt.plot(history.history['accuracy'], label='train_accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss')
plt.legend()
plt.show()

test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=1)

print('test loss: ',test_loss)
print('test acc: ', test_acc)

"""THANK YOU!!!"""

